{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e00c3b0",
   "metadata": {},
   "source": [
    "**Yong Zhu Cheng A0275768H**\n",
    "\n",
    "The objective of the project is to train a machine to generate an empathetic response to a given prompt, leveraging the large EmpatheticDialogues dataset developed by Rashkin et al.(2018). The dataset consists of crowd-sourced one-on-one dialogues covering a range of emotions, with human-annotated emotions as a key feature.\n",
    "\n",
    "The model used \n",
    "\n",
    "## 1. Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9afcbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import tarfile\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertGenerationEncoder, BertGenerationDecoder, EncoderDecoderModel\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from transformers import Seq2SeqTrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "import evaluate\n",
    "import warnings\n",
    "import gc\n",
    "\n",
    "main_path = r'C:\\Users\\yongz\\NUS\\DSA5202_Project'\n",
    "os.chdir(main_path)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d44e4de",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing\n",
    "\n",
    "\n",
    "\n",
    "The following cells, up to the indicated checkpoint, are only run once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680fcf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [RUN ONCE] Extract zip file\n",
    "# data_path = r'dataset'\n",
    "# file = tarfile.open('empatheticdialogues.tar.gz')\n",
    "# if os.path.isdir(data_path) is False:\n",
    "#     os.mkdir(data_path)\n",
    "#     file.extractall(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25830e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(r'dataset\\empatheticdialogues\\train.csv',on_bad_lines='skip')\n",
    "test_df = pd.read_csv(r'dataset\\empatheticdialogues\\test.csv',on_bad_lines='skip')\n",
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3078cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfb75eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prev_utterance(row,df):\n",
    "    if row.utterance_idx == 1:\n",
    "        return pd.NA\n",
    "    else:\n",
    "        return df.at[row.conv_id+'_'+str(int(row.utterance_idx)-1),'utterance']\n",
    "\n",
    "def process_df(df0,save_dir):    \n",
    "    df = df0.copy()\n",
    "    df['index1'] = df['conv_id'] + '_' + df['utterance_idx'].astype(str)\n",
    "    df = df.set_index('index1',drop=True)\n",
    "    conv_ids = df['conv_id'].unique()\n",
    "    for conv_idx in tqdm(conv_ids):\n",
    "        u_ids = [df.at[i,'utterance_idx'] for i in df.index if conv_idx in i]\n",
    "        if max(u_ids) != len(u_ids):\n",
    "            df = df.drop(df[df['conv_id']==conv_idx].index)\n",
    "    df['utterance0'] = df.apply(lambda row: get_prev_utterance(row,df),axis=1)\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = df[['context','prompt','utterance0','utterance']]\n",
    "    df = df.rename(columns={'utterance':'utterance1'})\n",
    "    df = df.dropna(subset='utterance0')\n",
    "    df.to_csv(save_dir,index=False)\n",
    "    \n",
    "process_df(train_df,r'dataset\\empatheticdialogues\\train_v1.csv')\n",
    "process_df(test_df,r'dataset\\empatheticdialogues\\test_v1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3decc0e8",
   "metadata": {},
   "source": [
    "Resume from here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98eb7bd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>prompt</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sentimental</td>\n",
       "      <td>I remember going to the fireworks with my best...</td>\n",
       "      <td>I remember going to see the fireworks with my ...</td>\n",
       "      <td>Was this a friend you were in love with_comma_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sentimental</td>\n",
       "      <td>I remember going to the fireworks with my best...</td>\n",
       "      <td>Was this a friend you were in love with_comma_...</td>\n",
       "      <td>This was a best friend. I miss her.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sentimental</td>\n",
       "      <td>I remember going to the fireworks with my best...</td>\n",
       "      <td>This was a best friend. I miss her.</td>\n",
       "      <td>Where has she gone?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sentimental</td>\n",
       "      <td>I remember going to the fireworks with my best...</td>\n",
       "      <td>Where has she gone?</td>\n",
       "      <td>We no longer talk.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sentimental</td>\n",
       "      <td>I remember going to the fireworks with my best...</td>\n",
       "      <td>We no longer talk.</td>\n",
       "      <td>Oh was this something that happened because of...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       context                                             prompt  \\\n",
       "0  sentimental  I remember going to the fireworks with my best...   \n",
       "1  sentimental  I remember going to the fireworks with my best...   \n",
       "2  sentimental  I remember going to the fireworks with my best...   \n",
       "3  sentimental  I remember going to the fireworks with my best...   \n",
       "4  sentimental  I remember going to the fireworks with my best...   \n",
       "\n",
       "                                                text  \\\n",
       "0  I remember going to see the fireworks with my ...   \n",
       "1  Was this a friend you were in love with_comma_...   \n",
       "2                This was a best friend. I miss her.   \n",
       "3                                Where has she gone?   \n",
       "4                                 We no longer talk.   \n",
       "\n",
       "                                               label  \n",
       "0  Was this a friend you were in love with_comma_...  \n",
       "1                This was a best friend. I miss her.  \n",
       "2                                Where has she gone?  \n",
       "3                                 We no longer talk.  \n",
       "4  Oh was this something that happened because of...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df1 = pd.read_csv(r'dataset\\empatheticdialogues\\train_v1.csv')\n",
    "test_df1 = pd.read_csv(r'dataset\\empatheticdialogues\\test_v1.csv')\n",
    "obj_del = ['enc','dec','model','tokenizer']\n",
    "train_df1 = train_df1.rename(columns={'utterance0':'text','utterance1':'label'})\n",
    "train_df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b2c223",
   "metadata": {},
   "source": [
    "Model 1: BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa4bbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = 'facebook/bart-base'\n",
    "for obj in obj_del:\n",
    "    if obj in globals(): del globals()[obj]\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_checkpoint,do_lower_case=True)\n",
    "def tokenize(sample):\n",
    "    return tokenizer(sample['text'],text_target=sample['label'],max_length=200,padding='max_length',truncation=True)\n",
    "\n",
    "core_ds = Dataset.from_pandas(train_df1[['text','label']])\n",
    "core_ds = core_ds.map(tokenize,batched=True,remove_columns=core_ds.column_names)\n",
    "core_ds = core_ds.train_test_split(test_size=0.2,seed=10)\n",
    "train_ds = core_ds['train'].select(range(2000)) # for smaller subset\n",
    "val_ds =  core_ds['test'].select(range(1000))\n",
    "enc = BertGenerationEncoder.from_pretrained(model_checkpoint)\n",
    "dec = BertGenerationDecoder.from_pretrained(model_checkpoint, add_cross_attention=True, is_decoder=True, bos_token_id=101, eos_token_id=102)\n",
    "model = EncoderDecoderModel(encoder=enc,decoder=dec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde8ebc0",
   "metadata": {},
   "source": [
    "Model 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d5a40ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c76149c4f33444ae966aaf81cd1d81cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/58770 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_checkpoint = 'facebook/bart-base'\n",
    "for obj in obj_del:\n",
    "    if obj in globals(): del globals()[obj]\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint,do_lower_case=True)\n",
    "def tokenize(sample):\n",
    "    return tokenizer(sample['text'],text_target=sample['label'],max_length=200,padding='max_length',truncation=True)\n",
    "\n",
    "core_ds = Dataset.from_pandas(train_df1[['text','label']])\n",
    "core_ds = core_ds.map(tokenize,batched=True,remove_columns=core_ds.column_names)\n",
    "core_ds = core_ds.train_test_split(test_size=0.2,seed=10)\n",
    "train_ds = core_ds['train'].select(range(2000)) # for smaller subset\n",
    "val_ds =  core_ds['test'].select(range(1000))\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e98a245",
   "metadata": {},
   "outputs": [],
   "source": [
    "for obj in obj_del:\n",
    "    if obj in globals(): del globals()[obj]\n",
    "train_ds = core_ds['train'].select(range(2000)) # for smaller subset\n",
    "val_ds =  core_ds['test'].select(range(1000))\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bb9d771",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = r'models'\n",
    "if os.path.isdir(model_dir) is False:\n",
    "    os.mkdir(model_dir)\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=model_dir,\n",
    "    disable_tqdm=False,\n",
    "    num_train_epochs=5,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "#     label_names=['utterance1']\n",
    ") # finetune hp\n",
    "training_args.set_logging(report_to=['tensorboard'])\n",
    "training_args.set_dataloader(train_batch_size=2,eval_batch_size=2)\n",
    "\n",
    "model.config.decoder_start_token_id = model.config.bos_token_id\n",
    "model.config.pad_token_id = -100 # https://discuss.huggingface.co/t/expected-workflow-100-and-padding-in-labels-in-seq2seq/27692\n",
    "\n",
    "metric=evaluate.load('bleu')\n",
    "def compute_metrics(eval_pred):\n",
    "    pred,labels = eval_pred\n",
    "    return metric.compute(predictions=pred,references=labels)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986de6ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='55' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 55/100 11:52 < 09:53, 0.08 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gc.collect()\n",
    "val_ds2 = core_ds['test'].select(range(200))\n",
    "trainer.evaluate(val_ds2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30186f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1000722b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Naive approach:\n",
    "Fine-tune pre-trained model on successive sentences (y_t-1 -> y_t)\n",
    "Generate empathetic reply to a response\n",
    "\n",
    "Limitations:\n",
    "No nuance in representing empathy, e.g. different words, etc.\n",
    "No theoretical framework\n",
    "\n",
    "Scope for improvement:\n",
    "Variables:\n",
    "Decoder-only\n",
    "W/ or w/o cross attention\n",
    "W/ other information encoded\n",
    "Hyperparameters\n",
    "'''\n",
    "no_print=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d690193",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c0e4d5",
   "metadata": {},
   "source": [
    "https://github.com/facebookresearch/EmpatheticDialogues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9838b9ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env",
   "language": "python",
   "name": "my-conda-env-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
