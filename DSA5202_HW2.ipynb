{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3050d37",
   "metadata": {},
   "source": [
    "## Yong Zhu Cheng A0275768H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bf2e919",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import ModuleList\n",
    "from torch.nn import functional as F\n",
    "from tqdm.auto import tqdm\n",
    "from torchsummary import summary\n",
    "import gc\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else: device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3abe2e8",
   "metadata": {},
   "source": [
    "### Script for model size evaluation\n",
    "Adapted from pytorch_modelsize by jacobkimmel. Significant modifications needed to accommodate Transformer logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39a34cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "    \n",
    "class SizeEstimator(object):\n",
    "\n",
    "    def __init__(self, model, input_size=(1,1,32,32), bits=32):\n",
    "        '''\n",
    "        Estimates the size of PyTorch models in memory\n",
    "        for a given input size\n",
    "        '''\n",
    "        self.model = model\n",
    "        self.input_size = input_size\n",
    "        self.bits = bits\n",
    "        return\n",
    "\n",
    "    def get_parameter_sizes(self):\n",
    "        '''Get sizes of all parameters in `model`'''\n",
    "        mods = list(self.model.modules())\n",
    "        sizes = []\n",
    "        \n",
    "        for i in range(1,len(mods)):\n",
    "            m = mods[i]\n",
    "            p = list(m.parameters())\n",
    "            for j in range(len(p)):\n",
    "                sizes.append(np.array(p[j].size()))\n",
    "\n",
    "        self.param_sizes = sizes\n",
    "        return\n",
    "\n",
    "    def get_output_sizes(self):\n",
    "        '''Run sample input through each layer to get output sizes'''\n",
    "        input_ = Variable(torch.FloatTensor(*self.input_size).to(device)).to(device)\n",
    "        target_ = Variable(torch.FloatTensor(*self.input_size).to(device)).to(device)\n",
    "        mods = list(self.model.named_modules())\n",
    "        out_sizes = []\n",
    "        for i in tqdm(range(1, len(mods))):\n",
    "            name = mods[i][0]\n",
    "            name_list = name.split('.')\n",
    "            m = mods[i][1]\n",
    "\n",
    "            # Special logic for specific layers\n",
    "            if isinstance(m,ModuleList):\n",
    "                if name_list[-1] == 'layers':\n",
    "                    if 'encoder' in name_list:\n",
    "                        for L in m:\n",
    "                            out = L(input_)\n",
    "                            out_sizes.append(np.array(out.size()))\n",
    "                    elif 'decoder' in name_list:\n",
    "                        for L in m:\n",
    "                            memory_ = Variable(torch.FloatTensor(*self.memory_size).to(device)).to(device)\n",
    "                            out = L(input_,memory_)\n",
    "                            out_sizes.append(np.array(out.size()))\n",
    "                elif 'mha1_e' in name_list or 'mha1_d' in name_list:\n",
    "                    for L in m:\n",
    "                        out = L(input_,input_,input_)\n",
    "                        out_sizes.append(np.array(out.size()))\n",
    "                elif 'mha2_d' in name_list and name_list[-1] == 'heads':\n",
    "                    memory_ = Variable(torch.FloatTensor(*self.memory_size).to(device)).to(device)\n",
    "                    for L in m:\n",
    "                        out = L(input_,memory_,memory_)\n",
    "                        out_sizes.append(np.array(out.size()))\n",
    "            elif isinstance(m,nn.Linear) == False and ('mha1_e' in name_list or 'mha1_d' in name_list):\n",
    "                out = m(input_,input_,input_)\n",
    "                out_sizes.append(np.array(out.size()))\n",
    "            elif (isinstance(m,nn.Linear) == False and 'mha2_d' in name_list):\n",
    "                memory_ = Variable(torch.FloatTensor(*self.memory_size).to(device)).to(device)\n",
    "                out = m(input_,memory_,memory_)\n",
    "                out_sizes.append(np.array(out.size()))\n",
    "            elif name_list[-1]=='decoder' or ('decoder' in name_list and name_list[-2]=='layers'):\n",
    "                memory_ = Variable(torch.FloatTensor(*self.memory_size).to(device)).to(device)\n",
    "                out = m(input_,memory_)\n",
    "                out_sizes.append(np.array(out.size()))\n",
    "            else:\n",
    "                out = m(input_)\n",
    "                out_sizes.append(np.array(out.size()))\n",
    "            \n",
    "            # Remembering other special layer sizes\n",
    "            if name_list[-1] == 'encoder':\n",
    "                self.memory_size = out.size()\n",
    "            \n",
    "            # The inputs for some layers should not be updated, e.g. in a module list\n",
    "            if name_list[-1] != 'heads' and (name_list[-1].isdigit() is False or name_list[-2] != 'heads') and not (name_list[-1] in ['Wk','Wq','Wv']):\n",
    "                input_ = out\n",
    "        \n",
    "        self.out_sizes = out_sizes\n",
    "        return\n",
    "\n",
    "    def calc_param_bits(self):\n",
    "        '''Calculate total number of bits to store `model` parameters'''\n",
    "        total_bits = 0\n",
    "        total_bits = np.int64(total_bits)\n",
    "        for i in range(len(self.param_sizes)):\n",
    "            s = self.param_sizes[i]\n",
    "            bits = np.int64(np.prod(np.array(s))*self.bits)\n",
    "            total_bits += bits\n",
    "        self.param_bits = total_bits\n",
    "        return\n",
    "\n",
    "    def calc_forward_backward_bits(self):\n",
    "        '''Calculate bits to store forward and backward pass'''\n",
    "        total_bits = 0\n",
    "        total_bits = np.int64(total_bits)\n",
    "        for i in range(len(self.out_sizes)):\n",
    "            s = self.out_sizes[i]\n",
    "            result = 1\n",
    "            for n in s:\n",
    "                result *= n\n",
    "            bits = result*self.bits\n",
    "            bits = np.int64(bits)\n",
    "            total_bits += bits\n",
    "        # multiply by 2 for both forward AND backward\n",
    "        self.forward_backward_bits = (total_bits*2)\n",
    "        return\n",
    "\n",
    "    def calc_input_bits(self):\n",
    "        '''Calculate bits to store input'''\n",
    "        self.input_bits = np.prod(np.array(self.input_size))*self.bits\n",
    "        return\n",
    "\n",
    "    def estimate_size(self):\n",
    "        '''Estimate model size in memory in megabytes and bits'''\n",
    "        self.get_parameter_sizes()\n",
    "        self.get_output_sizes()\n",
    "        self.calc_param_bits()\n",
    "        self.calc_forward_backward_bits()\n",
    "        self.calc_input_bits()\n",
    "        total = self.param_bits + self.forward_backward_bits + self.input_bits\n",
    "        total_megabytes = (total/8)/(1024**2)\n",
    "        return total_megabytes, total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc04d1f1",
   "metadata": {},
   "source": [
    "### Build Model - Transformer\n",
    "Building blocks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80c6e3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self,Q,K,V,mask=None,dropout=None):\n",
    "        '''\n",
    "        input shape: B*L*H\n",
    "        H: hidden layer dim, i.e. 'model_size'\n",
    "        L: sequence length\n",
    "        '''\n",
    "        out = torch.matmul(Q,K.transpose(1,2)) # shape: B*L*L\n",
    "        out = out / (Q.shape[-1]**0.5)\n",
    "        out = F.softmax(out,dim=-1)\n",
    "        return torch.matmul(out,V)\n",
    "        \n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, model_size,qkv_size):\n",
    "        super().__init__()\n",
    "        self.Wq = nn.Linear(model_size,qkv_size)\n",
    "        self.Wk = nn.Linear(model_size,qkv_size)\n",
    "        self.Wv = nn.Linear(model_size,qkv_size)\n",
    "        self.attention = Attention()\n",
    "    \n",
    "    def forward(self,queries,keys,values):\n",
    "        return self.attention(self.Wq(queries),\n",
    "                             self.Wk(keys),\n",
    "                             self.Wv(values))\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,num_heads,model_size,qkv_size):\n",
    "        super().__init__()\n",
    "        self.heads = ModuleList(\n",
    "            [AttentionHead(model_size,qkv_size) for _ in range(num_heads)]\n",
    "        )\n",
    "        self.Wo = nn.Linear(num_heads*qkv_size,model_size)\n",
    "        self.qkv_size = qkv_size\n",
    "        self.model_size = model_size\n",
    "    \n",
    "    def forward(self,query,key,value):\n",
    "        out_heads = [head(query,key,value) for head in self.heads]\n",
    "        out = torch.cat(out_heads, dim=-1)\n",
    "        return self.Wo(out)\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,model_size,hidden_size=2048):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(model_size,hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size,model_size),\n",
    "        )\n",
    "    \n",
    "    def forward(self,X):\n",
    "        return self.net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee104cc",
   "metadata": {},
   "source": [
    "Construct Encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bca959b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self,model_size,num_heads,ff_hidden_size,dropout):\n",
    "        super().__init__()\n",
    "        qkv_size = max(model_size//num_heads,1)\n",
    "        \n",
    "        # MHA\n",
    "        self.mha1_e = MultiHeadAttention(num_heads,model_size,qkv_size)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.norm1 = nn.LayerNorm(model_size) # LayerNorm vs BatchNorm\n",
    "        \n",
    "        # FF\n",
    "        self.ff = FeedForward(model_size,ff_hidden_size)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.norm2 = nn.LayerNorm(model_size)\n",
    "        \n",
    "    def forward(self,source):\n",
    "        # MHA (*Self-attention)\n",
    "        out1 = self.mha1_e(source,source,source)\n",
    "        out1 = self.dropout1(out1)\n",
    "        out1 = self.norm1(out1 + source)\n",
    "        \n",
    "        # FF\n",
    "        out2 = self.ff(out1)\n",
    "        out2 = self.dropout2(out2)\n",
    "        out2 = self.norm2(out2 + out1)\n",
    "        return out2\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                num_layers=6,\n",
    "                model_size=512,\n",
    "                num_heads=8,\n",
    "                ff_hidden_size=2048,\n",
    "                dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = ModuleList(\n",
    "            [TransformerEncoderLayer(model_size,\n",
    "                                    num_heads,\n",
    "                                    ff_hidden_size,\n",
    "                                    dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "    \n",
    "    def forward(self,source):\n",
    "        for L in self.layers:\n",
    "            source = L(source)\n",
    "        return source "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0fe2b1",
   "metadata": {},
   "source": [
    "Construct Decoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d82a1c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self,model_size,num_heads,ff_hidden_size,dropout):\n",
    "        super().__init__()\n",
    "        qkv_size = max(model_size//num_heads,1)\n",
    "        \n",
    "        # MHA 1\n",
    "        self.mha1_d = MultiHeadAttention(num_heads,model_size,qkv_size)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.norm1 = nn.LayerNorm(model_size) \n",
    "        \n",
    "        # MHA 2\n",
    "        self.mha2_d = MultiHeadAttention(num_heads,model_size,qkv_size)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.norm2 = nn.LayerNorm(model_size) \n",
    "        \n",
    "        # FF\n",
    "        self.ff = FeedForward(model_size,ff_hidden_size)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        self.norm3 = nn.LayerNorm(model_size)\n",
    "    \n",
    "    def forward(self,target,memory):\n",
    "        '''\n",
    "        memory: output of encoder\n",
    "        '''\n",
    "        # MHA 1 (Self-attention)\n",
    "        out1 = self.mha1_d(target,target,target)\n",
    "        out1 = self.dropout1(out1)\n",
    "        out1 = self.norm1(out1 + target)\n",
    "        \n",
    "        # MHA 2 (Source-target attention)\n",
    "        out2 = self.mha2_d(out1,memory,memory)\n",
    "        out2 = self.dropout2(out2)\n",
    "        out2 = self.norm2(out2 + out1)\n",
    "        \n",
    "        # FF\n",
    "        out3 = self.ff(out2)\n",
    "        out3 = self.dropout3(out3)\n",
    "        out3 = self.norm3(out3 + out2)\n",
    "        return out3\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                num_layers=6,\n",
    "                model_size=512,\n",
    "                num_heads=8,\n",
    "                ff_hidden_size=2048,\n",
    "                dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = ModuleList(\n",
    "            [TransformerDecoderLayer(model_size,\n",
    "                                    num_heads,\n",
    "                                    ff_hidden_size,\n",
    "                                    dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "    \n",
    "    def forward(self,target,memory):\n",
    "        for L in self.layers:\n",
    "            target = L(target,memory)\n",
    "        return target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f2fcc5",
   "metadata": {},
   "source": [
    "Complete Transformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a8ce485",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                num_encoder_layers=6,\n",
    "                num_decoder_layers=6,\n",
    "                model_size=512,\n",
    "                num_heads=8,\n",
    "                ff_hidden_size=2048,\n",
    "                dropout=0.1\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.encoder = TransformerEncoder(\n",
    "            num_layers=num_encoder_layers,\n",
    "            model_size=model_size,\n",
    "            num_heads=num_heads,\n",
    "            ff_hidden_size=ff_hidden_size,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.decoder = TransformerDecoder(\n",
    "            num_layers=num_decoder_layers,\n",
    "            model_size=model_size,\n",
    "            num_heads=num_heads,\n",
    "            ff_hidden_size=ff_hidden_size,\n",
    "            dropout=dropout\n",
    "        )\n",
    "    \n",
    "    def forward(self,source,target):\n",
    "        memory = self.encoder(source)\n",
    "        return self.decoder(target,memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc1de8d",
   "metadata": {},
   "source": [
    "### Generate model summary\n",
    "Some small modifications required for torchsummary code, primarily to accommodate multiple inputs to the Transformer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "daa91d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1              [-1, 200, 64]          32,832\n",
      "            Linear-2              [-1, 200, 64]          32,832\n",
      "            Linear-3              [-1, 200, 64]          32,832\n",
      "         Attention-4              [-1, 200, 64]               0\n",
      "     AttentionHead-5              [-1, 200, 64]               0\n",
      "            Linear-6              [-1, 200, 64]          32,832\n",
      "            Linear-7              [-1, 200, 64]          32,832\n",
      "            Linear-8              [-1, 200, 64]          32,832\n",
      "         Attention-9              [-1, 200, 64]               0\n",
      "    AttentionHead-10              [-1, 200, 64]               0\n",
      "           Linear-11              [-1, 200, 64]          32,832\n",
      "           Linear-12              [-1, 200, 64]          32,832\n",
      "           Linear-13              [-1, 200, 64]          32,832\n",
      "        Attention-14              [-1, 200, 64]               0\n",
      "    AttentionHead-15              [-1, 200, 64]               0\n",
      "           Linear-16              [-1, 200, 64]          32,832\n",
      "           Linear-17              [-1, 200, 64]          32,832\n",
      "           Linear-18              [-1, 200, 64]          32,832\n",
      "        Attention-19              [-1, 200, 64]               0\n",
      "    AttentionHead-20              [-1, 200, 64]               0\n",
      "           Linear-21              [-1, 200, 64]          32,832\n",
      "           Linear-22              [-1, 200, 64]          32,832\n",
      "           Linear-23              [-1, 200, 64]          32,832\n",
      "        Attention-24              [-1, 200, 64]               0\n",
      "    AttentionHead-25              [-1, 200, 64]               0\n",
      "           Linear-26              [-1, 200, 64]          32,832\n",
      "           Linear-27              [-1, 200, 64]          32,832\n",
      "           Linear-28              [-1, 200, 64]          32,832\n",
      "        Attention-29              [-1, 200, 64]               0\n",
      "    AttentionHead-30              [-1, 200, 64]               0\n",
      "           Linear-31              [-1, 200, 64]          32,832\n",
      "           Linear-32              [-1, 200, 64]          32,832\n",
      "           Linear-33              [-1, 200, 64]          32,832\n",
      "        Attention-34              [-1, 200, 64]               0\n",
      "    AttentionHead-35              [-1, 200, 64]               0\n",
      "           Linear-36              [-1, 200, 64]          32,832\n",
      "           Linear-37              [-1, 200, 64]          32,832\n",
      "           Linear-38              [-1, 200, 64]          32,832\n",
      "        Attention-39              [-1, 200, 64]               0\n",
      "    AttentionHead-40              [-1, 200, 64]               0\n",
      "           Linear-41             [-1, 200, 512]         262,656\n",
      "MultiHeadAttention-42             [-1, 200, 512]               0\n",
      "          Dropout-43             [-1, 200, 512]               0\n",
      "        LayerNorm-44             [-1, 200, 512]           1,024\n",
      "           Linear-45            [-1, 200, 2048]       1,050,624\n",
      "             ReLU-46            [-1, 200, 2048]               0\n",
      "           Linear-47             [-1, 200, 512]       1,049,088\n",
      "      FeedForward-48             [-1, 200, 512]               0\n",
      "          Dropout-49             [-1, 200, 512]               0\n",
      "        LayerNorm-50             [-1, 200, 512]           1,024\n",
      "TransformerEncoderLayer-51             [-1, 200, 512]               0\n",
      "           Linear-52              [-1, 200, 64]          32,832\n",
      "           Linear-53              [-1, 200, 64]          32,832\n",
      "           Linear-54              [-1, 200, 64]          32,832\n",
      "        Attention-55              [-1, 200, 64]               0\n",
      "    AttentionHead-56              [-1, 200, 64]               0\n",
      "           Linear-57              [-1, 200, 64]          32,832\n",
      "           Linear-58              [-1, 200, 64]          32,832\n",
      "           Linear-59              [-1, 200, 64]          32,832\n",
      "        Attention-60              [-1, 200, 64]               0\n",
      "    AttentionHead-61              [-1, 200, 64]               0\n",
      "           Linear-62              [-1, 200, 64]          32,832\n",
      "           Linear-63              [-1, 200, 64]          32,832\n",
      "           Linear-64              [-1, 200, 64]          32,832\n",
      "        Attention-65              [-1, 200, 64]               0\n",
      "    AttentionHead-66              [-1, 200, 64]               0\n",
      "           Linear-67              [-1, 200, 64]          32,832\n",
      "           Linear-68              [-1, 200, 64]          32,832\n",
      "           Linear-69              [-1, 200, 64]          32,832\n",
      "        Attention-70              [-1, 200, 64]               0\n",
      "    AttentionHead-71              [-1, 200, 64]               0\n",
      "           Linear-72              [-1, 200, 64]          32,832\n",
      "           Linear-73              [-1, 200, 64]          32,832\n",
      "           Linear-74              [-1, 200, 64]          32,832\n",
      "        Attention-75              [-1, 200, 64]               0\n",
      "    AttentionHead-76              [-1, 200, 64]               0\n",
      "           Linear-77              [-1, 200, 64]          32,832\n",
      "           Linear-78              [-1, 200, 64]          32,832\n",
      "           Linear-79              [-1, 200, 64]          32,832\n",
      "        Attention-80              [-1, 200, 64]               0\n",
      "    AttentionHead-81              [-1, 200, 64]               0\n",
      "           Linear-82              [-1, 200, 64]          32,832\n",
      "           Linear-83              [-1, 200, 64]          32,832\n",
      "           Linear-84              [-1, 200, 64]          32,832\n",
      "        Attention-85              [-1, 200, 64]               0\n",
      "    AttentionHead-86              [-1, 200, 64]               0\n",
      "           Linear-87              [-1, 200, 64]          32,832\n",
      "           Linear-88              [-1, 200, 64]          32,832\n",
      "           Linear-89              [-1, 200, 64]          32,832\n",
      "        Attention-90              [-1, 200, 64]               0\n",
      "    AttentionHead-91              [-1, 200, 64]               0\n",
      "           Linear-92             [-1, 200, 512]         262,656\n",
      "MultiHeadAttention-93             [-1, 200, 512]               0\n",
      "          Dropout-94             [-1, 200, 512]               0\n",
      "        LayerNorm-95             [-1, 200, 512]           1,024\n",
      "           Linear-96            [-1, 200, 2048]       1,050,624\n",
      "             ReLU-97            [-1, 200, 2048]               0\n",
      "           Linear-98             [-1, 200, 512]       1,049,088\n",
      "      FeedForward-99             [-1, 200, 512]               0\n",
      "         Dropout-100             [-1, 200, 512]               0\n",
      "       LayerNorm-101             [-1, 200, 512]           1,024\n",
      "TransformerEncoderLayer-102             [-1, 200, 512]               0\n",
      "          Linear-103              [-1, 200, 64]          32,832\n",
      "          Linear-104              [-1, 200, 64]          32,832\n",
      "          Linear-105              [-1, 200, 64]          32,832\n",
      "       Attention-106              [-1, 200, 64]               0\n",
      "   AttentionHead-107              [-1, 200, 64]               0\n",
      "          Linear-108              [-1, 200, 64]          32,832\n",
      "          Linear-109              [-1, 200, 64]          32,832\n",
      "          Linear-110              [-1, 200, 64]          32,832\n",
      "       Attention-111              [-1, 200, 64]               0\n",
      "   AttentionHead-112              [-1, 200, 64]               0\n",
      "          Linear-113              [-1, 200, 64]          32,832\n",
      "          Linear-114              [-1, 200, 64]          32,832\n",
      "          Linear-115              [-1, 200, 64]          32,832\n",
      "       Attention-116              [-1, 200, 64]               0\n",
      "   AttentionHead-117              [-1, 200, 64]               0\n",
      "          Linear-118              [-1, 200, 64]          32,832\n",
      "          Linear-119              [-1, 200, 64]          32,832\n",
      "          Linear-120              [-1, 200, 64]          32,832\n",
      "       Attention-121              [-1, 200, 64]               0\n",
      "   AttentionHead-122              [-1, 200, 64]               0\n",
      "          Linear-123              [-1, 200, 64]          32,832\n",
      "          Linear-124              [-1, 200, 64]          32,832\n",
      "          Linear-125              [-1, 200, 64]          32,832\n",
      "       Attention-126              [-1, 200, 64]               0\n",
      "   AttentionHead-127              [-1, 200, 64]               0\n",
      "          Linear-128              [-1, 200, 64]          32,832\n",
      "          Linear-129              [-1, 200, 64]          32,832\n",
      "          Linear-130              [-1, 200, 64]          32,832\n",
      "       Attention-131              [-1, 200, 64]               0\n",
      "   AttentionHead-132              [-1, 200, 64]               0\n",
      "          Linear-133              [-1, 200, 64]          32,832\n",
      "          Linear-134              [-1, 200, 64]          32,832\n",
      "          Linear-135              [-1, 200, 64]          32,832\n",
      "       Attention-136              [-1, 200, 64]               0\n",
      "   AttentionHead-137              [-1, 200, 64]               0\n",
      "          Linear-138              [-1, 200, 64]          32,832\n",
      "          Linear-139              [-1, 200, 64]          32,832\n",
      "          Linear-140              [-1, 200, 64]          32,832\n",
      "       Attention-141              [-1, 200, 64]               0\n",
      "   AttentionHead-142              [-1, 200, 64]               0\n",
      "          Linear-143             [-1, 200, 512]         262,656\n",
      "MultiHeadAttention-144             [-1, 200, 512]               0\n",
      "         Dropout-145             [-1, 200, 512]               0\n",
      "       LayerNorm-146             [-1, 200, 512]           1,024\n",
      "          Linear-147            [-1, 200, 2048]       1,050,624\n",
      "            ReLU-148            [-1, 200, 2048]               0\n",
      "          Linear-149             [-1, 200, 512]       1,049,088\n",
      "     FeedForward-150             [-1, 200, 512]               0\n",
      "         Dropout-151             [-1, 200, 512]               0\n",
      "       LayerNorm-152             [-1, 200, 512]           1,024\n",
      "TransformerEncoderLayer-153             [-1, 200, 512]               0\n",
      "          Linear-154              [-1, 200, 64]          32,832\n",
      "          Linear-155              [-1, 200, 64]          32,832\n",
      "          Linear-156              [-1, 200, 64]          32,832\n",
      "       Attention-157              [-1, 200, 64]               0\n",
      "   AttentionHead-158              [-1, 200, 64]               0\n",
      "          Linear-159              [-1, 200, 64]          32,832\n",
      "          Linear-160              [-1, 200, 64]          32,832\n",
      "          Linear-161              [-1, 200, 64]          32,832\n",
      "       Attention-162              [-1, 200, 64]               0\n",
      "   AttentionHead-163              [-1, 200, 64]               0\n",
      "          Linear-164              [-1, 200, 64]          32,832\n",
      "          Linear-165              [-1, 200, 64]          32,832\n",
      "          Linear-166              [-1, 200, 64]          32,832\n",
      "       Attention-167              [-1, 200, 64]               0\n",
      "   AttentionHead-168              [-1, 200, 64]               0\n",
      "          Linear-169              [-1, 200, 64]          32,832\n",
      "          Linear-170              [-1, 200, 64]          32,832\n",
      "          Linear-171              [-1, 200, 64]          32,832\n",
      "       Attention-172              [-1, 200, 64]               0\n",
      "   AttentionHead-173              [-1, 200, 64]               0\n",
      "          Linear-174              [-1, 200, 64]          32,832\n",
      "          Linear-175              [-1, 200, 64]          32,832\n",
      "          Linear-176              [-1, 200, 64]          32,832\n",
      "       Attention-177              [-1, 200, 64]               0\n",
      "   AttentionHead-178              [-1, 200, 64]               0\n",
      "          Linear-179              [-1, 200, 64]          32,832\n",
      "          Linear-180              [-1, 200, 64]          32,832\n",
      "          Linear-181              [-1, 200, 64]          32,832\n",
      "       Attention-182              [-1, 200, 64]               0\n",
      "   AttentionHead-183              [-1, 200, 64]               0\n",
      "          Linear-184              [-1, 200, 64]          32,832\n",
      "          Linear-185              [-1, 200, 64]          32,832\n",
      "          Linear-186              [-1, 200, 64]          32,832\n",
      "       Attention-187              [-1, 200, 64]               0\n",
      "   AttentionHead-188              [-1, 200, 64]               0\n",
      "          Linear-189              [-1, 200, 64]          32,832\n",
      "          Linear-190              [-1, 200, 64]          32,832\n",
      "          Linear-191              [-1, 200, 64]          32,832\n",
      "       Attention-192              [-1, 200, 64]               0\n",
      "   AttentionHead-193              [-1, 200, 64]               0\n",
      "          Linear-194             [-1, 200, 512]         262,656\n",
      "MultiHeadAttention-195             [-1, 200, 512]               0\n",
      "         Dropout-196             [-1, 200, 512]               0\n",
      "       LayerNorm-197             [-1, 200, 512]           1,024\n",
      "          Linear-198            [-1, 200, 2048]       1,050,624\n",
      "            ReLU-199            [-1, 200, 2048]               0\n",
      "          Linear-200             [-1, 200, 512]       1,049,088\n",
      "     FeedForward-201             [-1, 200, 512]               0\n",
      "         Dropout-202             [-1, 200, 512]               0\n",
      "       LayerNorm-203             [-1, 200, 512]           1,024\n",
      "TransformerEncoderLayer-204             [-1, 200, 512]               0\n",
      "          Linear-205              [-1, 200, 64]          32,832\n",
      "          Linear-206              [-1, 200, 64]          32,832\n",
      "          Linear-207              [-1, 200, 64]          32,832\n",
      "       Attention-208              [-1, 200, 64]               0\n",
      "   AttentionHead-209              [-1, 200, 64]               0\n",
      "          Linear-210              [-1, 200, 64]          32,832\n",
      "          Linear-211              [-1, 200, 64]          32,832\n",
      "          Linear-212              [-1, 200, 64]          32,832\n",
      "       Attention-213              [-1, 200, 64]               0\n",
      "   AttentionHead-214              [-1, 200, 64]               0\n",
      "          Linear-215              [-1, 200, 64]          32,832\n",
      "          Linear-216              [-1, 200, 64]          32,832\n",
      "          Linear-217              [-1, 200, 64]          32,832\n",
      "       Attention-218              [-1, 200, 64]               0\n",
      "   AttentionHead-219              [-1, 200, 64]               0\n",
      "          Linear-220              [-1, 200, 64]          32,832\n",
      "          Linear-221              [-1, 200, 64]          32,832\n",
      "          Linear-222              [-1, 200, 64]          32,832\n",
      "       Attention-223              [-1, 200, 64]               0\n",
      "   AttentionHead-224              [-1, 200, 64]               0\n",
      "          Linear-225              [-1, 200, 64]          32,832\n",
      "          Linear-226              [-1, 200, 64]          32,832\n",
      "          Linear-227              [-1, 200, 64]          32,832\n",
      "       Attention-228              [-1, 200, 64]               0\n",
      "   AttentionHead-229              [-1, 200, 64]               0\n",
      "          Linear-230              [-1, 200, 64]          32,832\n",
      "          Linear-231              [-1, 200, 64]          32,832\n",
      "          Linear-232              [-1, 200, 64]          32,832\n",
      "       Attention-233              [-1, 200, 64]               0\n",
      "   AttentionHead-234              [-1, 200, 64]               0\n",
      "          Linear-235              [-1, 200, 64]          32,832\n",
      "          Linear-236              [-1, 200, 64]          32,832\n",
      "          Linear-237              [-1, 200, 64]          32,832\n",
      "       Attention-238              [-1, 200, 64]               0\n",
      "   AttentionHead-239              [-1, 200, 64]               0\n",
      "          Linear-240              [-1, 200, 64]          32,832\n",
      "          Linear-241              [-1, 200, 64]          32,832\n",
      "          Linear-242              [-1, 200, 64]          32,832\n",
      "       Attention-243              [-1, 200, 64]               0\n",
      "   AttentionHead-244              [-1, 200, 64]               0\n",
      "          Linear-245             [-1, 200, 512]         262,656\n",
      "MultiHeadAttention-246             [-1, 200, 512]               0\n",
      "         Dropout-247             [-1, 200, 512]               0\n",
      "       LayerNorm-248             [-1, 200, 512]           1,024\n",
      "          Linear-249            [-1, 200, 2048]       1,050,624\n",
      "            ReLU-250            [-1, 200, 2048]               0\n",
      "          Linear-251             [-1, 200, 512]       1,049,088\n",
      "     FeedForward-252             [-1, 200, 512]               0\n",
      "         Dropout-253             [-1, 200, 512]               0\n",
      "       LayerNorm-254             [-1, 200, 512]           1,024\n",
      "TransformerEncoderLayer-255             [-1, 200, 512]               0\n",
      "          Linear-256              [-1, 200, 64]          32,832\n",
      "          Linear-257              [-1, 200, 64]          32,832\n",
      "          Linear-258              [-1, 200, 64]          32,832\n",
      "       Attention-259              [-1, 200, 64]               0\n",
      "   AttentionHead-260              [-1, 200, 64]               0\n",
      "          Linear-261              [-1, 200, 64]          32,832\n",
      "          Linear-262              [-1, 200, 64]          32,832\n",
      "          Linear-263              [-1, 200, 64]          32,832\n",
      "       Attention-264              [-1, 200, 64]               0\n",
      "   AttentionHead-265              [-1, 200, 64]               0\n",
      "          Linear-266              [-1, 200, 64]          32,832\n",
      "          Linear-267              [-1, 200, 64]          32,832\n",
      "          Linear-268              [-1, 200, 64]          32,832\n",
      "       Attention-269              [-1, 200, 64]               0\n",
      "   AttentionHead-270              [-1, 200, 64]               0\n",
      "          Linear-271              [-1, 200, 64]          32,832\n",
      "          Linear-272              [-1, 200, 64]          32,832\n",
      "          Linear-273              [-1, 200, 64]          32,832\n",
      "       Attention-274              [-1, 200, 64]               0\n",
      "   AttentionHead-275              [-1, 200, 64]               0\n",
      "          Linear-276              [-1, 200, 64]          32,832\n",
      "          Linear-277              [-1, 200, 64]          32,832\n",
      "          Linear-278              [-1, 200, 64]          32,832\n",
      "       Attention-279              [-1, 200, 64]               0\n",
      "   AttentionHead-280              [-1, 200, 64]               0\n",
      "          Linear-281              [-1, 200, 64]          32,832\n",
      "          Linear-282              [-1, 200, 64]          32,832\n",
      "          Linear-283              [-1, 200, 64]          32,832\n",
      "       Attention-284              [-1, 200, 64]               0\n",
      "   AttentionHead-285              [-1, 200, 64]               0\n",
      "          Linear-286              [-1, 200, 64]          32,832\n",
      "          Linear-287              [-1, 200, 64]          32,832\n",
      "          Linear-288              [-1, 200, 64]          32,832\n",
      "       Attention-289              [-1, 200, 64]               0\n",
      "   AttentionHead-290              [-1, 200, 64]               0\n",
      "          Linear-291              [-1, 200, 64]          32,832\n",
      "          Linear-292              [-1, 200, 64]          32,832\n",
      "          Linear-293              [-1, 200, 64]          32,832\n",
      "       Attention-294              [-1, 200, 64]               0\n",
      "   AttentionHead-295              [-1, 200, 64]               0\n",
      "          Linear-296             [-1, 200, 512]         262,656\n",
      "MultiHeadAttention-297             [-1, 200, 512]               0\n",
      "         Dropout-298             [-1, 200, 512]               0\n",
      "       LayerNorm-299             [-1, 200, 512]           1,024\n",
      "          Linear-300            [-1, 200, 2048]       1,050,624\n",
      "            ReLU-301            [-1, 200, 2048]               0\n",
      "          Linear-302             [-1, 200, 512]       1,049,088\n",
      "     FeedForward-303             [-1, 200, 512]               0\n",
      "         Dropout-304             [-1, 200, 512]               0\n",
      "       LayerNorm-305             [-1, 200, 512]           1,024\n",
      "TransformerEncoderLayer-306             [-1, 200, 512]               0\n",
      "TransformerEncoder-307             [-1, 200, 512]               0\n",
      "          Linear-308              [-1, 200, 64]          32,832\n",
      "          Linear-309              [-1, 200, 64]          32,832\n",
      "          Linear-310              [-1, 200, 64]          32,832\n",
      "       Attention-311              [-1, 200, 64]               0\n",
      "   AttentionHead-312              [-1, 200, 64]               0\n",
      "          Linear-313              [-1, 200, 64]          32,832\n",
      "          Linear-314              [-1, 200, 64]          32,832\n",
      "          Linear-315              [-1, 200, 64]          32,832\n",
      "       Attention-316              [-1, 200, 64]               0\n",
      "   AttentionHead-317              [-1, 200, 64]               0\n",
      "          Linear-318              [-1, 200, 64]          32,832\n",
      "          Linear-319              [-1, 200, 64]          32,832\n",
      "          Linear-320              [-1, 200, 64]          32,832\n",
      "       Attention-321              [-1, 200, 64]               0\n",
      "   AttentionHead-322              [-1, 200, 64]               0\n",
      "          Linear-323              [-1, 200, 64]          32,832\n",
      "          Linear-324              [-1, 200, 64]          32,832\n",
      "          Linear-325              [-1, 200, 64]          32,832\n",
      "       Attention-326              [-1, 200, 64]               0\n",
      "   AttentionHead-327              [-1, 200, 64]               0\n",
      "          Linear-328              [-1, 200, 64]          32,832\n",
      "          Linear-329              [-1, 200, 64]          32,832\n",
      "          Linear-330              [-1, 200, 64]          32,832\n",
      "       Attention-331              [-1, 200, 64]               0\n",
      "   AttentionHead-332              [-1, 200, 64]               0\n",
      "          Linear-333              [-1, 200, 64]          32,832\n",
      "          Linear-334              [-1, 200, 64]          32,832\n",
      "          Linear-335              [-1, 200, 64]          32,832\n",
      "       Attention-336              [-1, 200, 64]               0\n",
      "   AttentionHead-337              [-1, 200, 64]               0\n",
      "          Linear-338              [-1, 200, 64]          32,832\n",
      "          Linear-339              [-1, 200, 64]          32,832\n",
      "          Linear-340              [-1, 200, 64]          32,832\n",
      "       Attention-341              [-1, 200, 64]               0\n",
      "   AttentionHead-342              [-1, 200, 64]               0\n",
      "          Linear-343              [-1, 200, 64]          32,832\n",
      "          Linear-344              [-1, 200, 64]          32,832\n",
      "          Linear-345              [-1, 200, 64]          32,832\n",
      "       Attention-346              [-1, 200, 64]               0\n",
      "   AttentionHead-347              [-1, 200, 64]               0\n",
      "          Linear-348             [-1, 200, 512]         262,656\n",
      "MultiHeadAttention-349             [-1, 200, 512]               0\n",
      "         Dropout-350             [-1, 200, 512]               0\n",
      "       LayerNorm-351             [-1, 200, 512]           1,024\n",
      "          Linear-352              [-1, 200, 64]          32,832\n",
      "          Linear-353              [-1, 200, 64]          32,832\n",
      "          Linear-354              [-1, 200, 64]          32,832\n",
      "       Attention-355              [-1, 200, 64]               0\n",
      "   AttentionHead-356              [-1, 200, 64]               0\n",
      "          Linear-357              [-1, 200, 64]          32,832\n",
      "          Linear-358              [-1, 200, 64]          32,832\n",
      "          Linear-359              [-1, 200, 64]          32,832\n",
      "       Attention-360              [-1, 200, 64]               0\n",
      "   AttentionHead-361              [-1, 200, 64]               0\n",
      "          Linear-362              [-1, 200, 64]          32,832\n",
      "          Linear-363              [-1, 200, 64]          32,832\n",
      "          Linear-364              [-1, 200, 64]          32,832\n",
      "       Attention-365              [-1, 200, 64]               0\n",
      "   AttentionHead-366              [-1, 200, 64]               0\n",
      "          Linear-367              [-1, 200, 64]          32,832\n",
      "          Linear-368              [-1, 200, 64]          32,832\n",
      "          Linear-369              [-1, 200, 64]          32,832\n",
      "       Attention-370              [-1, 200, 64]               0\n",
      "   AttentionHead-371              [-1, 200, 64]               0\n",
      "          Linear-372              [-1, 200, 64]          32,832\n",
      "          Linear-373              [-1, 200, 64]          32,832\n",
      "          Linear-374              [-1, 200, 64]          32,832\n",
      "       Attention-375              [-1, 200, 64]               0\n",
      "   AttentionHead-376              [-1, 200, 64]               0\n",
      "          Linear-377              [-1, 200, 64]          32,832\n",
      "          Linear-378              [-1, 200, 64]          32,832\n",
      "          Linear-379              [-1, 200, 64]          32,832\n",
      "       Attention-380              [-1, 200, 64]               0\n",
      "   AttentionHead-381              [-1, 200, 64]               0\n",
      "          Linear-382              [-1, 200, 64]          32,832\n",
      "          Linear-383              [-1, 200, 64]          32,832\n",
      "          Linear-384              [-1, 200, 64]          32,832\n",
      "       Attention-385              [-1, 200, 64]               0\n",
      "   AttentionHead-386              [-1, 200, 64]               0\n",
      "          Linear-387              [-1, 200, 64]          32,832\n",
      "          Linear-388              [-1, 200, 64]          32,832\n",
      "          Linear-389              [-1, 200, 64]          32,832\n",
      "       Attention-390              [-1, 200, 64]               0\n",
      "   AttentionHead-391              [-1, 200, 64]               0\n",
      "          Linear-392             [-1, 200, 512]         262,656\n",
      "MultiHeadAttention-393             [-1, 200, 512]               0\n",
      "         Dropout-394             [-1, 200, 512]               0\n",
      "       LayerNorm-395             [-1, 200, 512]           1,024\n",
      "          Linear-396            [-1, 200, 2048]       1,050,624\n",
      "            ReLU-397            [-1, 200, 2048]               0\n",
      "          Linear-398             [-1, 200, 512]       1,049,088\n",
      "     FeedForward-399             [-1, 200, 512]               0\n",
      "         Dropout-400             [-1, 200, 512]               0\n",
      "       LayerNorm-401             [-1, 200, 512]           1,024\n",
      "TransformerDecoderLayer-402             [-1, 200, 512]               0\n",
      "          Linear-403              [-1, 200, 64]          32,832\n",
      "          Linear-404              [-1, 200, 64]          32,832\n",
      "          Linear-405              [-1, 200, 64]          32,832\n",
      "       Attention-406              [-1, 200, 64]               0\n",
      "   AttentionHead-407              [-1, 200, 64]               0\n",
      "          Linear-408              [-1, 200, 64]          32,832\n",
      "          Linear-409              [-1, 200, 64]          32,832\n",
      "          Linear-410              [-1, 200, 64]          32,832\n",
      "       Attention-411              [-1, 200, 64]               0\n",
      "   AttentionHead-412              [-1, 200, 64]               0\n",
      "          Linear-413              [-1, 200, 64]          32,832\n",
      "          Linear-414              [-1, 200, 64]          32,832\n",
      "          Linear-415              [-1, 200, 64]          32,832\n",
      "       Attention-416              [-1, 200, 64]               0\n",
      "   AttentionHead-417              [-1, 200, 64]               0\n",
      "          Linear-418              [-1, 200, 64]          32,832\n",
      "          Linear-419              [-1, 200, 64]          32,832\n",
      "          Linear-420              [-1, 200, 64]          32,832\n",
      "       Attention-421              [-1, 200, 64]               0\n",
      "   AttentionHead-422              [-1, 200, 64]               0\n",
      "          Linear-423              [-1, 200, 64]          32,832\n",
      "          Linear-424              [-1, 200, 64]          32,832\n",
      "          Linear-425              [-1, 200, 64]          32,832\n",
      "       Attention-426              [-1, 200, 64]               0\n",
      "   AttentionHead-427              [-1, 200, 64]               0\n",
      "          Linear-428              [-1, 200, 64]          32,832\n",
      "          Linear-429              [-1, 200, 64]          32,832\n",
      "          Linear-430              [-1, 200, 64]          32,832\n",
      "       Attention-431              [-1, 200, 64]               0\n",
      "   AttentionHead-432              [-1, 200, 64]               0\n",
      "          Linear-433              [-1, 200, 64]          32,832\n",
      "          Linear-434              [-1, 200, 64]          32,832\n",
      "          Linear-435              [-1, 200, 64]          32,832\n",
      "       Attention-436              [-1, 200, 64]               0\n",
      "   AttentionHead-437              [-1, 200, 64]               0\n",
      "          Linear-438              [-1, 200, 64]          32,832\n",
      "          Linear-439              [-1, 200, 64]          32,832\n",
      "          Linear-440              [-1, 200, 64]          32,832\n",
      "       Attention-441              [-1, 200, 64]               0\n",
      "   AttentionHead-442              [-1, 200, 64]               0\n",
      "          Linear-443             [-1, 200, 512]         262,656\n",
      "MultiHeadAttention-444             [-1, 200, 512]               0\n",
      "         Dropout-445             [-1, 200, 512]               0\n",
      "       LayerNorm-446             [-1, 200, 512]           1,024\n",
      "          Linear-447              [-1, 200, 64]          32,832\n",
      "          Linear-448              [-1, 200, 64]          32,832\n",
      "          Linear-449              [-1, 200, 64]          32,832\n",
      "       Attention-450              [-1, 200, 64]               0\n",
      "   AttentionHead-451              [-1, 200, 64]               0\n",
      "          Linear-452              [-1, 200, 64]          32,832\n",
      "          Linear-453              [-1, 200, 64]          32,832\n",
      "          Linear-454              [-1, 200, 64]          32,832\n",
      "       Attention-455              [-1, 200, 64]               0\n",
      "   AttentionHead-456              [-1, 200, 64]               0\n",
      "          Linear-457              [-1, 200, 64]          32,832\n",
      "          Linear-458              [-1, 200, 64]          32,832\n",
      "          Linear-459              [-1, 200, 64]          32,832\n",
      "       Attention-460              [-1, 200, 64]               0\n",
      "   AttentionHead-461              [-1, 200, 64]               0\n",
      "          Linear-462              [-1, 200, 64]          32,832\n",
      "          Linear-463              [-1, 200, 64]          32,832\n",
      "          Linear-464              [-1, 200, 64]          32,832\n",
      "       Attention-465              [-1, 200, 64]               0\n",
      "   AttentionHead-466              [-1, 200, 64]               0\n",
      "          Linear-467              [-1, 200, 64]          32,832\n",
      "          Linear-468              [-1, 200, 64]          32,832\n",
      "          Linear-469              [-1, 200, 64]          32,832\n",
      "       Attention-470              [-1, 200, 64]               0\n",
      "   AttentionHead-471              [-1, 200, 64]               0\n",
      "          Linear-472              [-1, 200, 64]          32,832\n",
      "          Linear-473              [-1, 200, 64]          32,832\n",
      "          Linear-474              [-1, 200, 64]          32,832\n",
      "       Attention-475              [-1, 200, 64]               0\n",
      "   AttentionHead-476              [-1, 200, 64]               0\n",
      "          Linear-477              [-1, 200, 64]          32,832\n",
      "          Linear-478              [-1, 200, 64]          32,832\n",
      "          Linear-479              [-1, 200, 64]          32,832\n",
      "       Attention-480              [-1, 200, 64]               0\n",
      "   AttentionHead-481              [-1, 200, 64]               0\n",
      "          Linear-482              [-1, 200, 64]          32,832\n",
      "          Linear-483              [-1, 200, 64]          32,832\n",
      "          Linear-484              [-1, 200, 64]          32,832\n",
      "       Attention-485              [-1, 200, 64]               0\n",
      "   AttentionHead-486              [-1, 200, 64]               0\n",
      "          Linear-487             [-1, 200, 512]         262,656\n",
      "MultiHeadAttention-488             [-1, 200, 512]               0\n",
      "         Dropout-489             [-1, 200, 512]               0\n",
      "       LayerNorm-490             [-1, 200, 512]           1,024\n",
      "          Linear-491            [-1, 200, 2048]       1,050,624\n",
      "            ReLU-492            [-1, 200, 2048]               0\n",
      "          Linear-493             [-1, 200, 512]       1,049,088\n",
      "     FeedForward-494             [-1, 200, 512]               0\n",
      "         Dropout-495             [-1, 200, 512]               0\n",
      "       LayerNorm-496             [-1, 200, 512]           1,024\n",
      "TransformerDecoderLayer-497             [-1, 200, 512]               0\n",
      "          Linear-498              [-1, 200, 64]          32,832\n",
      "          Linear-499              [-1, 200, 64]          32,832\n",
      "          Linear-500              [-1, 200, 64]          32,832\n",
      "       Attention-501              [-1, 200, 64]               0\n",
      "   AttentionHead-502              [-1, 200, 64]               0\n",
      "          Linear-503              [-1, 200, 64]          32,832\n",
      "          Linear-504              [-1, 200, 64]          32,832\n",
      "          Linear-505              [-1, 200, 64]          32,832\n",
      "       Attention-506              [-1, 200, 64]               0\n",
      "   AttentionHead-507              [-1, 200, 64]               0\n",
      "          Linear-508              [-1, 200, 64]          32,832\n",
      "          Linear-509              [-1, 200, 64]          32,832\n",
      "          Linear-510              [-1, 200, 64]          32,832\n",
      "       Attention-511              [-1, 200, 64]               0\n",
      "   AttentionHead-512              [-1, 200, 64]               0\n",
      "          Linear-513              [-1, 200, 64]          32,832\n",
      "          Linear-514              [-1, 200, 64]          32,832\n",
      "          Linear-515              [-1, 200, 64]          32,832\n",
      "       Attention-516              [-1, 200, 64]               0\n",
      "   AttentionHead-517              [-1, 200, 64]               0\n",
      "          Linear-518              [-1, 200, 64]          32,832\n",
      "          Linear-519              [-1, 200, 64]          32,832\n",
      "          Linear-520              [-1, 200, 64]          32,832\n",
      "       Attention-521              [-1, 200, 64]               0\n",
      "   AttentionHead-522              [-1, 200, 64]               0\n",
      "          Linear-523              [-1, 200, 64]          32,832\n",
      "          Linear-524              [-1, 200, 64]          32,832\n",
      "          Linear-525              [-1, 200, 64]          32,832\n",
      "       Attention-526              [-1, 200, 64]               0\n",
      "   AttentionHead-527              [-1, 200, 64]               0\n",
      "          Linear-528              [-1, 200, 64]          32,832\n",
      "          Linear-529              [-1, 200, 64]          32,832\n",
      "          Linear-530              [-1, 200, 64]          32,832\n",
      "       Attention-531              [-1, 200, 64]               0\n",
      "   AttentionHead-532              [-1, 200, 64]               0\n",
      "          Linear-533              [-1, 200, 64]          32,832\n",
      "          Linear-534              [-1, 200, 64]          32,832\n",
      "          Linear-535              [-1, 200, 64]          32,832\n",
      "       Attention-536              [-1, 200, 64]               0\n",
      "   AttentionHead-537              [-1, 200, 64]               0\n",
      "          Linear-538             [-1, 200, 512]         262,656\n",
      "MultiHeadAttention-539             [-1, 200, 512]               0\n",
      "         Dropout-540             [-1, 200, 512]               0\n",
      "       LayerNorm-541             [-1, 200, 512]           1,024\n",
      "          Linear-542              [-1, 200, 64]          32,832\n",
      "          Linear-543              [-1, 200, 64]          32,832\n",
      "          Linear-544              [-1, 200, 64]          32,832\n",
      "       Attention-545              [-1, 200, 64]               0\n",
      "   AttentionHead-546              [-1, 200, 64]               0\n",
      "          Linear-547              [-1, 200, 64]          32,832\n",
      "          Linear-548              [-1, 200, 64]          32,832\n",
      "          Linear-549              [-1, 200, 64]          32,832\n",
      "       Attention-550              [-1, 200, 64]               0\n",
      "   AttentionHead-551              [-1, 200, 64]               0\n",
      "          Linear-552              [-1, 200, 64]          32,832\n",
      "          Linear-553              [-1, 200, 64]          32,832\n",
      "          Linear-554              [-1, 200, 64]          32,832\n",
      "       Attention-555              [-1, 200, 64]               0\n",
      "   AttentionHead-556              [-1, 200, 64]               0\n",
      "          Linear-557              [-1, 200, 64]          32,832\n",
      "          Linear-558              [-1, 200, 64]          32,832\n",
      "          Linear-559              [-1, 200, 64]          32,832\n",
      "       Attention-560              [-1, 200, 64]               0\n",
      "   AttentionHead-561              [-1, 200, 64]               0\n",
      "          Linear-562              [-1, 200, 64]          32,832\n",
      "          Linear-563              [-1, 200, 64]          32,832\n",
      "          Linear-564              [-1, 200, 64]          32,832\n",
      "       Attention-565              [-1, 200, 64]               0\n",
      "   AttentionHead-566              [-1, 200, 64]               0\n",
      "          Linear-567              [-1, 200, 64]          32,832\n",
      "          Linear-568              [-1, 200, 64]          32,832\n",
      "          Linear-569              [-1, 200, 64]          32,832\n",
      "       Attention-570              [-1, 200, 64]               0\n",
      "   AttentionHead-571              [-1, 200, 64]               0\n",
      "          Linear-572              [-1, 200, 64]          32,832\n",
      "          Linear-573              [-1, 200, 64]          32,832\n",
      "          Linear-574              [-1, 200, 64]          32,832\n",
      "       Attention-575              [-1, 200, 64]               0\n",
      "   AttentionHead-576              [-1, 200, 64]               0\n",
      "          Linear-577              [-1, 200, 64]          32,832\n",
      "          Linear-578              [-1, 200, 64]          32,832\n",
      "          Linear-579              [-1, 200, 64]          32,832\n",
      "       Attention-580              [-1, 200, 64]               0\n",
      "   AttentionHead-581              [-1, 200, 64]               0\n",
      "          Linear-582             [-1, 200, 512]         262,656\n",
      "MultiHeadAttention-583             [-1, 200, 512]               0\n",
      "         Dropout-584             [-1, 200, 512]               0\n",
      "       LayerNorm-585             [-1, 200, 512]           1,024\n",
      "          Linear-586            [-1, 200, 2048]       1,050,624\n",
      "            ReLU-587            [-1, 200, 2048]               0\n",
      "          Linear-588             [-1, 200, 512]       1,049,088\n",
      "     FeedForward-589             [-1, 200, 512]               0\n",
      "         Dropout-590             [-1, 200, 512]               0\n",
      "       LayerNorm-591             [-1, 200, 512]           1,024\n",
      "TransformerDecoderLayer-592             [-1, 200, 512]               0\n",
      "          Linear-593              [-1, 200, 64]          32,832\n",
      "          Linear-594              [-1, 200, 64]          32,832\n",
      "          Linear-595              [-1, 200, 64]          32,832\n",
      "       Attention-596              [-1, 200, 64]               0\n",
      "   AttentionHead-597              [-1, 200, 64]               0\n",
      "          Linear-598              [-1, 200, 64]          32,832\n",
      "          Linear-599              [-1, 200, 64]          32,832\n",
      "          Linear-600              [-1, 200, 64]          32,832\n",
      "       Attention-601              [-1, 200, 64]               0\n",
      "   AttentionHead-602              [-1, 200, 64]               0\n",
      "          Linear-603              [-1, 200, 64]          32,832\n",
      "          Linear-604              [-1, 200, 64]          32,832\n",
      "          Linear-605              [-1, 200, 64]          32,832\n",
      "       Attention-606              [-1, 200, 64]               0\n",
      "   AttentionHead-607              [-1, 200, 64]               0\n",
      "          Linear-608              [-1, 200, 64]          32,832\n",
      "          Linear-609              [-1, 200, 64]          32,832\n",
      "          Linear-610              [-1, 200, 64]          32,832\n",
      "       Attention-611              [-1, 200, 64]               0\n",
      "   AttentionHead-612              [-1, 200, 64]               0\n",
      "          Linear-613              [-1, 200, 64]          32,832\n",
      "          Linear-614              [-1, 200, 64]          32,832\n",
      "          Linear-615              [-1, 200, 64]          32,832\n",
      "       Attention-616              [-1, 200, 64]               0\n",
      "   AttentionHead-617              [-1, 200, 64]               0\n",
      "          Linear-618              [-1, 200, 64]          32,832\n",
      "          Linear-619              [-1, 200, 64]          32,832\n",
      "          Linear-620              [-1, 200, 64]          32,832\n",
      "       Attention-621              [-1, 200, 64]               0\n",
      "   AttentionHead-622              [-1, 200, 64]               0\n",
      "          Linear-623              [-1, 200, 64]          32,832\n",
      "          Linear-624              [-1, 200, 64]          32,832\n",
      "          Linear-625              [-1, 200, 64]          32,832\n",
      "       Attention-626              [-1, 200, 64]               0\n",
      "   AttentionHead-627              [-1, 200, 64]               0\n",
      "          Linear-628              [-1, 200, 64]          32,832\n",
      "          Linear-629              [-1, 200, 64]          32,832\n",
      "          Linear-630              [-1, 200, 64]          32,832\n",
      "       Attention-631              [-1, 200, 64]               0\n",
      "   AttentionHead-632              [-1, 200, 64]               0\n",
      "          Linear-633             [-1, 200, 512]         262,656\n",
      "MultiHeadAttention-634             [-1, 200, 512]               0\n",
      "         Dropout-635             [-1, 200, 512]               0\n",
      "       LayerNorm-636             [-1, 200, 512]           1,024\n",
      "          Linear-637              [-1, 200, 64]          32,832\n",
      "          Linear-638              [-1, 200, 64]          32,832\n",
      "          Linear-639              [-1, 200, 64]          32,832\n",
      "       Attention-640              [-1, 200, 64]               0\n",
      "   AttentionHead-641              [-1, 200, 64]               0\n",
      "          Linear-642              [-1, 200, 64]          32,832\n",
      "          Linear-643              [-1, 200, 64]          32,832\n",
      "          Linear-644              [-1, 200, 64]          32,832\n",
      "       Attention-645              [-1, 200, 64]               0\n",
      "   AttentionHead-646              [-1, 200, 64]               0\n",
      "          Linear-647              [-1, 200, 64]          32,832\n",
      "          Linear-648              [-1, 200, 64]          32,832\n",
      "          Linear-649              [-1, 200, 64]          32,832\n",
      "       Attention-650              [-1, 200, 64]               0\n",
      "   AttentionHead-651              [-1, 200, 64]               0\n",
      "          Linear-652              [-1, 200, 64]          32,832\n",
      "          Linear-653              [-1, 200, 64]          32,832\n",
      "          Linear-654              [-1, 200, 64]          32,832\n",
      "       Attention-655              [-1, 200, 64]               0\n",
      "   AttentionHead-656              [-1, 200, 64]               0\n",
      "          Linear-657              [-1, 200, 64]          32,832\n",
      "          Linear-658              [-1, 200, 64]          32,832\n",
      "          Linear-659              [-1, 200, 64]          32,832\n",
      "       Attention-660              [-1, 200, 64]               0\n",
      "   AttentionHead-661              [-1, 200, 64]               0\n",
      "          Linear-662              [-1, 200, 64]          32,832\n",
      "          Linear-663              [-1, 200, 64]          32,832\n",
      "          Linear-664              [-1, 200, 64]          32,832\n",
      "       Attention-665              [-1, 200, 64]               0\n",
      "   AttentionHead-666              [-1, 200, 64]               0\n",
      "          Linear-667              [-1, 200, 64]          32,832\n",
      "          Linear-668              [-1, 200, 64]          32,832\n",
      "          Linear-669              [-1, 200, 64]          32,832\n",
      "       Attention-670              [-1, 200, 64]               0\n",
      "   AttentionHead-671              [-1, 200, 64]               0\n",
      "          Linear-672              [-1, 200, 64]          32,832\n",
      "          Linear-673              [-1, 200, 64]          32,832\n",
      "          Linear-674              [-1, 200, 64]          32,832\n",
      "       Attention-675              [-1, 200, 64]               0\n",
      "   AttentionHead-676              [-1, 200, 64]               0\n",
      "          Linear-677             [-1, 200, 512]         262,656\n",
      "MultiHeadAttention-678             [-1, 200, 512]               0\n",
      "         Dropout-679             [-1, 200, 512]               0\n",
      "       LayerNorm-680             [-1, 200, 512]           1,024\n",
      "          Linear-681            [-1, 200, 2048]       1,050,624\n",
      "            ReLU-682            [-1, 200, 2048]               0\n",
      "          Linear-683             [-1, 200, 512]       1,049,088\n",
      "     FeedForward-684             [-1, 200, 512]               0\n",
      "         Dropout-685             [-1, 200, 512]               0\n",
      "       LayerNorm-686             [-1, 200, 512]           1,024\n",
      "TransformerDecoderLayer-687             [-1, 200, 512]               0\n",
      "          Linear-688              [-1, 200, 64]          32,832\n",
      "          Linear-689              [-1, 200, 64]          32,832\n",
      "          Linear-690              [-1, 200, 64]          32,832\n",
      "       Attention-691              [-1, 200, 64]               0\n",
      "   AttentionHead-692              [-1, 200, 64]               0\n",
      "          Linear-693              [-1, 200, 64]          32,832\n",
      "          Linear-694              [-1, 200, 64]          32,832\n",
      "          Linear-695              [-1, 200, 64]          32,832\n",
      "       Attention-696              [-1, 200, 64]               0\n",
      "   AttentionHead-697              [-1, 200, 64]               0\n",
      "          Linear-698              [-1, 200, 64]          32,832\n",
      "          Linear-699              [-1, 200, 64]          32,832\n",
      "          Linear-700              [-1, 200, 64]          32,832\n",
      "       Attention-701              [-1, 200, 64]               0\n",
      "   AttentionHead-702              [-1, 200, 64]               0\n",
      "          Linear-703              [-1, 200, 64]          32,832\n",
      "          Linear-704              [-1, 200, 64]          32,832\n",
      "          Linear-705              [-1, 200, 64]          32,832\n",
      "       Attention-706              [-1, 200, 64]               0\n",
      "   AttentionHead-707              [-1, 200, 64]               0\n",
      "          Linear-708              [-1, 200, 64]          32,832\n",
      "          Linear-709              [-1, 200, 64]          32,832\n",
      "          Linear-710              [-1, 200, 64]          32,832\n",
      "       Attention-711              [-1, 200, 64]               0\n",
      "   AttentionHead-712              [-1, 200, 64]               0\n",
      "          Linear-713              [-1, 200, 64]          32,832\n",
      "          Linear-714              [-1, 200, 64]          32,832\n",
      "          Linear-715              [-1, 200, 64]          32,832\n",
      "       Attention-716              [-1, 200, 64]               0\n",
      "   AttentionHead-717              [-1, 200, 64]               0\n",
      "          Linear-718              [-1, 200, 64]          32,832\n",
      "          Linear-719              [-1, 200, 64]          32,832\n",
      "          Linear-720              [-1, 200, 64]          32,832\n",
      "       Attention-721              [-1, 200, 64]               0\n",
      "   AttentionHead-722              [-1, 200, 64]               0\n",
      "          Linear-723              [-1, 200, 64]          32,832\n",
      "          Linear-724              [-1, 200, 64]          32,832\n",
      "          Linear-725              [-1, 200, 64]          32,832\n",
      "       Attention-726              [-1, 200, 64]               0\n",
      "   AttentionHead-727              [-1, 200, 64]               0\n",
      "          Linear-728             [-1, 200, 512]         262,656\n",
      "MultiHeadAttention-729             [-1, 200, 512]               0\n",
      "         Dropout-730             [-1, 200, 512]               0\n",
      "       LayerNorm-731             [-1, 200, 512]           1,024\n",
      "          Linear-732              [-1, 200, 64]          32,832\n",
      "          Linear-733              [-1, 200, 64]          32,832\n",
      "          Linear-734              [-1, 200, 64]          32,832\n",
      "       Attention-735              [-1, 200, 64]               0\n",
      "   AttentionHead-736              [-1, 200, 64]               0\n",
      "          Linear-737              [-1, 200, 64]          32,832\n",
      "          Linear-738              [-1, 200, 64]          32,832\n",
      "          Linear-739              [-1, 200, 64]          32,832\n",
      "       Attention-740              [-1, 200, 64]               0\n",
      "   AttentionHead-741              [-1, 200, 64]               0\n",
      "          Linear-742              [-1, 200, 64]          32,832\n",
      "          Linear-743              [-1, 200, 64]          32,832\n",
      "          Linear-744              [-1, 200, 64]          32,832\n",
      "       Attention-745              [-1, 200, 64]               0\n",
      "   AttentionHead-746              [-1, 200, 64]               0\n",
      "          Linear-747              [-1, 200, 64]          32,832\n",
      "          Linear-748              [-1, 200, 64]          32,832\n",
      "          Linear-749              [-1, 200, 64]          32,832\n",
      "       Attention-750              [-1, 200, 64]               0\n",
      "   AttentionHead-751              [-1, 200, 64]               0\n",
      "          Linear-752              [-1, 200, 64]          32,832\n",
      "          Linear-753              [-1, 200, 64]          32,832\n",
      "          Linear-754              [-1, 200, 64]          32,832\n",
      "       Attention-755              [-1, 200, 64]               0\n",
      "   AttentionHead-756              [-1, 200, 64]               0\n",
      "          Linear-757              [-1, 200, 64]          32,832\n",
      "          Linear-758              [-1, 200, 64]          32,832\n",
      "          Linear-759              [-1, 200, 64]          32,832\n",
      "       Attention-760              [-1, 200, 64]               0\n",
      "   AttentionHead-761              [-1, 200, 64]               0\n",
      "          Linear-762              [-1, 200, 64]          32,832\n",
      "          Linear-763              [-1, 200, 64]          32,832\n",
      "          Linear-764              [-1, 200, 64]          32,832\n",
      "       Attention-765              [-1, 200, 64]               0\n",
      "   AttentionHead-766              [-1, 200, 64]               0\n",
      "          Linear-767              [-1, 200, 64]          32,832\n",
      "          Linear-768              [-1, 200, 64]          32,832\n",
      "          Linear-769              [-1, 200, 64]          32,832\n",
      "       Attention-770              [-1, 200, 64]               0\n",
      "   AttentionHead-771              [-1, 200, 64]               0\n",
      "          Linear-772             [-1, 200, 512]         262,656\n",
      "MultiHeadAttention-773             [-1, 200, 512]               0\n",
      "         Dropout-774             [-1, 200, 512]               0\n",
      "       LayerNorm-775             [-1, 200, 512]           1,024\n",
      "          Linear-776            [-1, 200, 2048]       1,050,624\n",
      "            ReLU-777            [-1, 200, 2048]               0\n",
      "          Linear-778             [-1, 200, 512]       1,049,088\n",
      "     FeedForward-779             [-1, 200, 512]               0\n",
      "         Dropout-780             [-1, 200, 512]               0\n",
      "       LayerNorm-781             [-1, 200, 512]           1,024\n",
      "TransformerDecoderLayer-782             [-1, 200, 512]               0\n",
      "          Linear-783              [-1, 200, 64]          32,832\n",
      "          Linear-784              [-1, 200, 64]          32,832\n",
      "          Linear-785              [-1, 200, 64]          32,832\n",
      "       Attention-786              [-1, 200, 64]               0\n",
      "   AttentionHead-787              [-1, 200, 64]               0\n",
      "          Linear-788              [-1, 200, 64]          32,832\n",
      "          Linear-789              [-1, 200, 64]          32,832\n",
      "          Linear-790              [-1, 200, 64]          32,832\n",
      "       Attention-791              [-1, 200, 64]               0\n",
      "   AttentionHead-792              [-1, 200, 64]               0\n",
      "          Linear-793              [-1, 200, 64]          32,832\n",
      "          Linear-794              [-1, 200, 64]          32,832\n",
      "          Linear-795              [-1, 200, 64]          32,832\n",
      "       Attention-796              [-1, 200, 64]               0\n",
      "   AttentionHead-797              [-1, 200, 64]               0\n",
      "          Linear-798              [-1, 200, 64]          32,832\n",
      "          Linear-799              [-1, 200, 64]          32,832\n",
      "          Linear-800              [-1, 200, 64]          32,832\n",
      "       Attention-801              [-1, 200, 64]               0\n",
      "   AttentionHead-802              [-1, 200, 64]               0\n",
      "          Linear-803              [-1, 200, 64]          32,832\n",
      "          Linear-804              [-1, 200, 64]          32,832\n",
      "          Linear-805              [-1, 200, 64]          32,832\n",
      "       Attention-806              [-1, 200, 64]               0\n",
      "   AttentionHead-807              [-1, 200, 64]               0\n",
      "          Linear-808              [-1, 200, 64]          32,832\n",
      "          Linear-809              [-1, 200, 64]          32,832\n",
      "          Linear-810              [-1, 200, 64]          32,832\n",
      "       Attention-811              [-1, 200, 64]               0\n",
      "   AttentionHead-812              [-1, 200, 64]               0\n",
      "          Linear-813              [-1, 200, 64]          32,832\n",
      "          Linear-814              [-1, 200, 64]          32,832\n",
      "          Linear-815              [-1, 200, 64]          32,832\n",
      "       Attention-816              [-1, 200, 64]               0\n",
      "   AttentionHead-817              [-1, 200, 64]               0\n",
      "          Linear-818              [-1, 200, 64]          32,832\n",
      "          Linear-819              [-1, 200, 64]          32,832\n",
      "          Linear-820              [-1, 200, 64]          32,832\n",
      "       Attention-821              [-1, 200, 64]               0\n",
      "   AttentionHead-822              [-1, 200, 64]               0\n",
      "          Linear-823             [-1, 200, 512]         262,656\n",
      "MultiHeadAttention-824             [-1, 200, 512]               0\n",
      "         Dropout-825             [-1, 200, 512]               0\n",
      "       LayerNorm-826             [-1, 200, 512]           1,024\n",
      "          Linear-827              [-1, 200, 64]          32,832\n",
      "          Linear-828              [-1, 200, 64]          32,832\n",
      "          Linear-829              [-1, 200, 64]          32,832\n",
      "       Attention-830              [-1, 200, 64]               0\n",
      "   AttentionHead-831              [-1, 200, 64]               0\n",
      "          Linear-832              [-1, 200, 64]          32,832\n",
      "          Linear-833              [-1, 200, 64]          32,832\n",
      "          Linear-834              [-1, 200, 64]          32,832\n",
      "       Attention-835              [-1, 200, 64]               0\n",
      "   AttentionHead-836              [-1, 200, 64]               0\n",
      "          Linear-837              [-1, 200, 64]          32,832\n",
      "          Linear-838              [-1, 200, 64]          32,832\n",
      "          Linear-839              [-1, 200, 64]          32,832\n",
      "       Attention-840              [-1, 200, 64]               0\n",
      "   AttentionHead-841              [-1, 200, 64]               0\n",
      "          Linear-842              [-1, 200, 64]          32,832\n",
      "          Linear-843              [-1, 200, 64]          32,832\n",
      "          Linear-844              [-1, 200, 64]          32,832\n",
      "       Attention-845              [-1, 200, 64]               0\n",
      "   AttentionHead-846              [-1, 200, 64]               0\n",
      "          Linear-847              [-1, 200, 64]          32,832\n",
      "          Linear-848              [-1, 200, 64]          32,832\n",
      "          Linear-849              [-1, 200, 64]          32,832\n",
      "       Attention-850              [-1, 200, 64]               0\n",
      "   AttentionHead-851              [-1, 200, 64]               0\n",
      "          Linear-852              [-1, 200, 64]          32,832\n",
      "          Linear-853              [-1, 200, 64]          32,832\n",
      "          Linear-854              [-1, 200, 64]          32,832\n",
      "       Attention-855              [-1, 200, 64]               0\n",
      "   AttentionHead-856              [-1, 200, 64]               0\n",
      "          Linear-857              [-1, 200, 64]          32,832\n",
      "          Linear-858              [-1, 200, 64]          32,832\n",
      "          Linear-859              [-1, 200, 64]          32,832\n",
      "       Attention-860              [-1, 200, 64]               0\n",
      "   AttentionHead-861              [-1, 200, 64]               0\n",
      "          Linear-862              [-1, 200, 64]          32,832\n",
      "          Linear-863              [-1, 200, 64]          32,832\n",
      "          Linear-864              [-1, 200, 64]          32,832\n",
      "       Attention-865              [-1, 200, 64]               0\n",
      "   AttentionHead-866              [-1, 200, 64]               0\n",
      "          Linear-867             [-1, 200, 512]         262,656\n",
      "MultiHeadAttention-868             [-1, 200, 512]               0\n",
      "         Dropout-869             [-1, 200, 512]               0\n",
      "       LayerNorm-870             [-1, 200, 512]           1,024\n",
      "          Linear-871            [-1, 200, 2048]       1,050,624\n",
      "            ReLU-872            [-1, 200, 2048]               0\n",
      "          Linear-873             [-1, 200, 512]       1,049,088\n",
      "     FeedForward-874             [-1, 200, 512]               0\n",
      "         Dropout-875             [-1, 200, 512]               0\n",
      "       LayerNorm-876             [-1, 200, 512]           1,024\n",
      "TransformerDecoderLayer-877             [-1, 200, 512]               0\n",
      "TransformerDecoder-878             [-1, 200, 512]               0\n",
      "================================================================\n",
      "Total params: 44,138,496\n",
      "Trainable params: 44,138,496\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 7232.00\n",
      "Forward/backward pass size (MB): 250.00\n",
      "Params size (MB): 168.38\n",
      "Estimated Total Size (MB): 7650.38\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "tf_model = Transformer().to(device)\n",
    "summary(tf_model,input_size=(200,512))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27ce8ca",
   "metadata": {},
   "source": [
    "### Evaluate model size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe45a036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7c3fe5619af4b4bb3baf5e51d2e150e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/910 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(7152.3359375, 59998142464)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if 'tf_model' in globals():\n",
    "    del tf_model\n",
    "gc.collect()\n",
    "\n",
    "tf_model = Transformer().to(device)\n",
    "se = SizeEstimator(tf_model,input_size=(16,200,512))\n",
    "se.estimate_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4229465a",
   "metadata": {},
   "source": [
    "**Estimated model size**: 7152.336 MB, 59998142464 bits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env",
   "language": "python",
   "name": "my-conda-env-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
