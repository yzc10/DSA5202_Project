{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd3ad17c",
   "metadata": {},
   "source": [
    "Yong Zhu Cheng A0275768H\n",
    "## 1. Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9afcbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import tarfile\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertGenerationEncoder, BertGenerationDecoder, EncoderDecoderModel\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from transformers import Seq2SeqTrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "import evaluate\n",
    "import warnings\n",
    "import gc\n",
    "\n",
    "main_path = r'C:\\Users\\yongz\\NUS\\DSA5202_Project'\n",
    "os.chdir(main_path)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575b0d3f",
   "metadata": {},
   "source": [
    "## 2. Preprocessing\n",
    "The following is only run once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680fcf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [RUN ONCE] Extract zip file\n",
    "# data_path = r'dataset'\n",
    "# file = tarfile.open('empatheticdialogues.tar.gz')\n",
    "# if os.path.isdir(data_path) is False:\n",
    "#     os.mkdir(data_path)\n",
    "#     file.extractall(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25830e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(r'dataset\\empatheticdialogues\\train.csv',on_bad_lines='skip')\n",
    "test_df = pd.read_csv(r'dataset\\empatheticdialogues\\test.csv',on_bad_lines='skip')\n",
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3078cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfb75eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prev_utterance(row,df):\n",
    "    if row.utterance_idx == 1:\n",
    "        return pd.NA\n",
    "    else:\n",
    "        return df.at[row.conv_id+'_'+str(int(row.utterance_idx)-1),'utterance']\n",
    "\n",
    "def process_df(df0,save_dir):    \n",
    "    df = df0.copy()\n",
    "    df['index1'] = df['conv_id'] + '_' + df['utterance_idx'].astype(str)\n",
    "    df = df.set_index('index1',drop=True)\n",
    "    conv_ids = df['conv_id'].unique()\n",
    "    for conv_idx in tqdm(conv_ids):\n",
    "        u_ids = [df.at[i,'utterance_idx'] for i in df.index if conv_idx in i]\n",
    "        if max(u_ids) != len(u_ids):\n",
    "            df = df.drop(df[df['conv_id']==conv_idx].index)\n",
    "    df['utterance0'] = df.apply(lambda row: get_prev_utterance(row,df),axis=1)\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = df[['context','prompt','utterance0','utterance']]\n",
    "    df = df.rename(columns={'utterance':'utterance1'})\n",
    "    df = df.dropna(subset='utterance0')\n",
    "    df.to_csv(save_dir,index=False)\n",
    "    \n",
    "process_df(train_df,r'dataset\\empatheticdialogues\\train_v1.csv')\n",
    "process_df(test_df,r'dataset\\empatheticdialogues\\test_v1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab50dd09",
   "metadata": {},
   "source": [
    "Resume from here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98eb7bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df1 = pd.read_csv(r'dataset\\empatheticdialogues\\train_v1.csv')\n",
    "test_df1 = pd.read_csv(r'dataset\\empatheticdialogues\\test_v1.csv')\n",
    "obj_del = ['enc','dec','model','tokenizer']\n",
    "train_df1 = train_df1.rename(columns={'utterance0':'text','utterance1':'label'})\n",
    "train_df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36624d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Steps:\n",
    "Convert to hf dataset\n",
    "Tokenize\n",
    "Train\n",
    "\n",
    "Variables:\n",
    "Decoder-only\n",
    "W/ or w/o cross attention\n",
    "W/ other information encoded\n",
    "Hyperparameters\n",
    "'''\n",
    "no_print=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7f0433",
   "metadata": {},
   "source": [
    "Model 1: BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3947152",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = 'facebook/bart-base'\n",
    "for obj in obj_del:\n",
    "    if obj in globals(): del globals()[obj]\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_checkpoint,do_lower_case=True)\n",
    "def tokenize(sample):\n",
    "    return tokenizer(sample['text'],text_target=sample['label'],max_length=200,padding='max_length',truncation=True)\n",
    "\n",
    "core_ds = Dataset.from_pandas(train_df1[['text','label']])\n",
    "core_ds = core_ds.map(tokenize,batched=True,remove_columns=core_ds.column_names)\n",
    "core_ds = core_ds.train_test_split(test_size=0.2,seed=10)\n",
    "train_ds = core_ds['train'].select(range(2000)) # for smaller subset\n",
    "val_ds =  core_ds['test'].select(range(1000))\n",
    "enc = BertGenerationEncoder.from_pretrained(model_checkpoint)\n",
    "dec = BertGenerationDecoder.from_pretrained(model_checkpoint, add_cross_attention=True, is_decoder=True, bos_token_id=101, eos_token_id=102)\n",
    "model = EncoderDecoderModel(encoder=enc,decoder=dec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f576864",
   "metadata": {},
   "source": [
    "Model 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ce1905",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_checkpoint = 'facebook/bart-base'\n",
    "for obj in obj_del:\n",
    "    if obj in globals(): del globals()[obj]\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint,do_lower_case=True)\n",
    "def tokenize(sample):\n",
    "    return tokenizer(sample['text'],text_target=sample['label'],max_length=200,padding='max_length',truncation=True)\n",
    "\n",
    "core_ds = Dataset.from_pandas(train_df1[['text','label']])\n",
    "core_ds = core_ds.map(tokenize,batched=True,remove_columns=core_ds.column_names)\n",
    "core_ds = core_ds.train_test_split(test_size=0.2,seed=10)\n",
    "train_ds = core_ds['train'].select(range(2000)).to(device) # for smaller subset\n",
    "val_ds =  core_ds['test'].select(range(1000)).to(device)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c70fc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "for obj in obj_del:\n",
    "    if obj in globals(): del globals()[obj]\n",
    "train_ds = core_ds['train'].select(range(2000)) # for smaller subset\n",
    "val_ds =  core_ds['test'].select(range(1000))\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff7790c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = r'models'\n",
    "if os.path.isdir(model_dir) is False:\n",
    "    os.mkdir(model_dir)\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=model_dir,\n",
    "    disable_tqdm=False,\n",
    "    num_train_epochs=5,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "#     label_names=['utterance1']\n",
    ") # finetune hp\n",
    "training_args.set_logging(report_to=['tensorboard'])\n",
    "training_args.set_dataloader(train_batch_size=2,eval_batch_size=2)\n",
    "\n",
    "model.config.decoder_start_token_id = model.config.bos_token_id\n",
    "model.config.pad_token_id = -100 # https://discuss.huggingface.co/t/expected-workflow-100-and-padding-in-labels-in-seq2seq/27692\n",
    "\n",
    "metric=evaluate.load('bleu')\n",
    "def compute_metrics(eval_pred):\n",
    "    pred,labels = eval_pred\n",
    "    return metric.compute(predictions=pred,references=labels)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa235ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds2 = core_ds['test'].select(range(200))\n",
    "gc.collect()\n",
    "trainer.evaluate(val_ds2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd65b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Naive approach:\n",
    "Fine-tune pre-trained model on successive sentences (y_t-1 -> y_t)\n",
    "Generate empathetic reply to a response\n",
    "\n",
    "Limitations:\n",
    "No nuance in representing empathy, e.g. different words, etc.\n",
    "No theoretical framework\n",
    "\n",
    "Scope for improvement:\n",
    "Including context\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env",
   "language": "python",
   "name": "my-conda-env-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
